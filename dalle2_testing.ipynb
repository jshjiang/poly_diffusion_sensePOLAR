{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175aebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d54e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71309ea68f4e4cc59d2cf02b26f091cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Decoder:', options=('Original', 'New 1B (Aesthetic)', 'New 1.5B (Aesthetiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd988c28bf943389af6d1b133ae7759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies\n",
      "Installing dalle2 version git+https://github.com/Veldrovive/DALLE2-pytorch@f4b687798d367fc434d8127ab31141f0fea0db26\n",
      "Do not worry if you get the error `fatal: destination path 'SwinIR' already exists and is not an empty directory.`\n",
      "That just means SwinIR is already installed and I'm too lazy to do the check myself\n",
      "fatal: destination path 'SwinIR' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "!pip install -q ipywidgets\n",
    "import ipywidgets as widgets\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "decoder_versions = [{\n",
    "    \"name\": \"Original\",\n",
    "    \"dalle2_install_path\": \"git+https://github.com/Veldrovive/DALLE2-pytorch@f4b687798d367fc434d8127ab31141f0fea0db26\",\n",
    "    \"decoder_path\": \"https://huggingface.co/Veldrovive/DA-VINC-E/resolve/main/text_conditioned_epoch_34.pth\",\n",
    "    \"config_path\": \"https://huggingface.co/Veldrovive/DA-VINC-E/raw/main/text_conditioned_config.json\"\n",
    "},{\n",
    "    \"name\": \"New 1B (Aesthetic)\",\n",
    "    \"dalle2_install_path\": \"dalle2_pytorch==0.15.4\",\n",
    "    \"decoder_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/resolve/main/decoder/small_32gpus/latest.pth\",\n",
    "    \"config_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/raw/main/decoder/small_32gpus/decoder_config.json\"\n",
    "},{\n",
    "    \"name\": \"New 1.5B (Aesthetic)\",\n",
    "    \"dalle2_install_path\": \"dalle2_pytorch==0.15.4\",\n",
    "    \"decoder_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/resolve/main/decoder/1.5B/latest.pth\",\n",
    "    \"config_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/raw/main/decoder/1.5B/decoder_config.json\"\n",
    "},{\n",
    "    \"name\": \"New 1.5B (Laion2B)\",\n",
    "    \"dalle2_install_path\": \"dalle2_pytorch==0.15.4\",\n",
    "    \"decoder_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/resolve/main/decoder/1.5B_laion2B/latest.pth\",\n",
    "    \"config_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/raw/main/decoder/1.5B_laion2B/decoder_config.json\"\n",
    "}]\n",
    "\n",
    "decoder_options = [version[\"name\"] for version in decoder_versions]\n",
    "\n",
    "def load_state():\n",
    "    state_path = \"script_state.json\"\n",
    "    try:\n",
    "        assert os.path.exists(state_path)\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        # Make sure the save config is up to date. You might think this is a stupid system but...\n",
    "        decoder = state[\"decoder\"]\n",
    "        if decoder is not None:\n",
    "          current_decoder_name = decoder[\"name\"]\n",
    "          try:\n",
    "              current_decoder_index = decoder_options.index(current_decoder_name)\n",
    "              state[\"decoder\"] = decoder_versions[current_decoder_index]\n",
    "          except ValueError:\n",
    "              print(\"The decoder you were using no longer exists. Please pick a new option.\")\n",
    "              state[\"decoder\"] = None\n",
    "        \n",
    "        # Check if models are where they say they are\n",
    "        for filekey in [\"decoder\", \"decoder_config\", \"prior\", \"prior_config\"]:\n",
    "            path = state[\"model_paths\"][filekey]\n",
    "            if path is not None and not os.path.exists(path):\n",
    "                print(f\"{filekey} not found in expected place. Removing decoder config.\")\n",
    "                state[\"decoder\"] = None\n",
    "                state[\"model_paths\"] = {\n",
    "                    \"decoder\": None,\n",
    "                    \"decoder_config\": None,\n",
    "                    \"prior\": None,\n",
    "                    \"prior_config\": None\n",
    "                }\n",
    "                save_state()\n",
    "    except Exception as e:\n",
    "        state = {\n",
    "            \"text_input\": '',\n",
    "            \"text_repeat\": 3,\n",
    "            \"prior_conditioning\": 1.0,\n",
    "            \"img_repeat\": 1,\n",
    "            \"decoder_conditioning\": 3.5,\n",
    "            \"include_prompt_checkbox\": True,\n",
    "            \"upsample_checkbox\": True,\n",
    "            \"decoder\": None,\n",
    "            \"model_paths\": {\n",
    "                \"decoder\": None,\n",
    "                \"decoder_config\": None,\n",
    "                \"prior\": None,\n",
    "                \"prior_config\": None\n",
    "            }\n",
    "        }\n",
    "    return state\n",
    "\n",
    "current_state = load_state()\n",
    "\n",
    "def save_state():\n",
    "    global current_state\n",
    "    state_path = \"script_state.json\"\n",
    "    with open(state_path, \"w\") as f:\n",
    "        json.dump(current_state, f)\n",
    "\n",
    "def choice_equal(new_choice):\n",
    "    global current_state\n",
    "    if current_state[\"decoder\"] is None:\n",
    "        return False\n",
    "    return current_state[\"decoder\"][\"decoder_path\"] == new_choice[\"decoder_path\"]\n",
    "\n",
    "def dalle2_imported():\n",
    "    return \"dalle2_pytorch\" in sys.modules\n",
    "\n",
    "chosen_decoder = current_state[\"decoder\"] if current_state[\"decoder\"] is not None else decoder_versions[-1]\n",
    "\n",
    "decoder_version_dropdown = widgets.Dropdown(\n",
    "    options=decoder_options,\n",
    "    value=chosen_decoder[\"name\"],\n",
    "    description='Decoder:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "start_setup_button = widgets.Button(\n",
    "    description=\"Setup\"\n",
    ")\n",
    "\n",
    "redownload_button = widgets.Button (\n",
    "    description=\"Force Update Models\"\n",
    ")\n",
    "\n",
    "main_layout = widgets.VBox([decoder_version_dropdown, start_setup_button, redownload_button])\n",
    "\n",
    "def setup(decoder_version_name):\n",
    "    global current_state\n",
    "    global chosen_decoder\n",
    "    new_choice = decoder_versions[decoder_options.index(decoder_version_name)]\n",
    "    current_choice = current_state[\"decoder\"]\n",
    "    \n",
    "    new_choice_equal = choice_equal(new_choice)\n",
    "    already_imported = dalle2_imported()\n",
    "    \n",
    "    requires_restart = not new_choice_equal and already_imported  # The wrong dalle2_pytorch version is already imported\n",
    "    requires_download = not new_choice_equal  # The wrong decoder version is downloaded\n",
    "    \n",
    "    print(f\"You are using the model {new_choice['name']} which will be downloaded from {new_choice['decoder_path']}\\n\")\n",
    "    if requires_restart:\n",
    "        print(\"You environment already has dalle2 imported and collab requires a restart for you to be able to import the new version.\")\n",
    "        print(\"Restart your runtime to proceed.\")\n",
    "    elif requires_download:\n",
    "        print(\"The models are not downloaded. They will be when you proceed.\")\n",
    "    else:\n",
    "        print(\"You are ready to run inference. If you suspect your models are out of date, force update them.\")\n",
    "    \n",
    "    chosen_decoder = new_choice\n",
    "    \n",
    "\n",
    "out = widgets.interactive_output(setup, { 'decoder_version_name': decoder_version_dropdown })\n",
    "display(main_layout, out)\n",
    "\n",
    "def download_models(current_choice):\n",
    "    model_dir = \"./models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Download decoder\n",
    "    print(\"Downloading decoder and decoder config\")\n",
    "    decoder_url = current_choice[\"decoder_path\"]\n",
    "    decoder_config_url = current_choice[\"config_path\"]\n",
    "\n",
    "    decoder_path = os.path.join(model_dir, \"decoder.pth\")\n",
    "    decoder_config_path = os.path.join(model_dir, \"decoder_config.json\")\n",
    "    \n",
    "    !curl -L {decoder_url} > {decoder_path}\n",
    "    !curl -L {decoder_config_url} > {decoder_config_path}\n",
    "    \n",
    "    # Download prior\n",
    "    print(\"Downloading prior and prior config\")\n",
    "    prior_url = \"https://huggingface.co/zenglishuci/conditioned-prior/resolve/main/vit-l-14/prior_aes_finetune.pth\"\n",
    "        \n",
    "    prior_path = os.path.join(model_dir, \"prior.pth\")\n",
    "    \n",
    "    !curl -L {prior_url} > {prior_path}\n",
    "    return decoder_path, decoder_config_path, prior_path, None\n",
    "\n",
    "def install_dependencies(state):\n",
    "    print(\"Installing dependencies\")\n",
    "    dalle2_install_path = state[\"decoder\"][\"dalle2_install_path\"]\n",
    "    print(f\"Installing dalle2 version {dalle2_install_path}\")\n",
    "    !pip install -q {dalle2_install_path}\n",
    "    \n",
    "    !pip install -q matplotlib\n",
    "\n",
    "    print(\"Do not worry if you get the error `fatal: destination path 'SwinIR' already exists and is not an empty directory.`\")\n",
    "    print(\"That just means SwinIR is already installed and I'm too lazy to do the check myself\")\n",
    "    !git clone https://github.com/JingyunLiang/SwinIR.git\n",
    "    !pip install -q timm\n",
    "    !pip install -q opencv-python\n",
    "\n",
    "def start_setup(b):\n",
    "    global current_state\n",
    "    global chosen_decoder\n",
    "    current_choice = current_state[\"decoder\"]\n",
    "    \n",
    "    new_choice_equal = choice_equal(chosen_decoder)\n",
    "    already_imported = dalle2_imported()\n",
    "    \n",
    "    requires_restart = not new_choice_equal and already_imported  # The wrong dalle2_pytorch version is already imported\n",
    "    requires_download = not new_choice_equal  # The wrong decoder version is downloaded\n",
    "    \n",
    "    if requires_restart:\n",
    "        raise Exception(\"Please restart your runtime before trying to set up the environment\")\n",
    "        \n",
    "    if requires_download:\n",
    "        try:\n",
    "            decoder_path, decoder_config_path, prior_path, prior_config_path = download_models(chosen_decoder)\n",
    "        except Exception as e:\n",
    "            print(\"Model download was interrupted. Manually delete all models or environment may be corrupted.\")\n",
    "            current_state[\"decoder\"] = None\n",
    "            save_state()\n",
    "            raise e\n",
    "        current_state[\"decoder\"] = chosen_decoder\n",
    "        current_state[\"model_paths\"] = {\n",
    "                \"decoder\": decoder_path,\n",
    "                \"decoder_config\": decoder_config_path,\n",
    "                \"prior\": prior_path,\n",
    "                \"prior_config\": prior_config_path\n",
    "            }\n",
    "        save_state()\n",
    "    \n",
    "    install_dependencies(current_state)\n",
    "\n",
    "start_setup_button.on_click(start_setup)\n",
    "\n",
    "def force_download(b):\n",
    "    global current_state\n",
    "    current_choice = current_state[\"decoder\"]\n",
    "    updated_choice = decoder_versions[decoder_options.index(current_choice[\"name\"])]\n",
    "    chosen_decoder = updated_choice\n",
    "    try:\n",
    "        decoder_path, decoder_config_path, prior_path, prior_config_path = download_models(chosen_decoder)\n",
    "    except Exception as e:\n",
    "        print(\"Model download was interrupted. Force update models or environment may be corrupted.\")\n",
    "        current_state[\"decoder\"] = None\n",
    "        save_state()\n",
    "        raise e\n",
    "    current_state[\"decoder\"] = chosen_decoder\n",
    "    current_state[\"model_paths\"] = {\n",
    "            \"decoder\": decoder_path,\n",
    "            \"decoder_config\": decoder_config_path,\n",
    "            \"prior\": prior_path,\n",
    "            \"prior_config\": prior_config_path\n",
    "        }\n",
    "    save_state()\n",
    "    install_dependencies(current_state)\n",
    "    \n",
    "redownload_button.on_click(force_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d1b8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# I would suggest running on a remote machine https://research.google.com/colaboratory/local-runtimes.html\n",
    "import shutil\n",
    "import torch\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "\n",
    "from dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, OpenAIClipAdapter, train_configs\n",
    "from dalle2_pytorch.tokenizer import tokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00db8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "def conditioned_on_text(config):\n",
    "    try:\n",
    "        return config.decoder.unets[0].cond_on_text_encodings\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        return config.decoder.condition_on_text_encodings\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "decoder_text_conditioned = False\n",
    "clip_config = None\n",
    "def load_decoder(decoder_state_dict_path, config_file_path):\n",
    "  config = train_configs.TrainDecoderConfig.from_json_path(config_file_path)\n",
    "  global decoder_text_conditioned\n",
    "  decoder_text_conditioned = conditioned_on_text(config)\n",
    "  global clip_config\n",
    "  clip_config = config.decoder.clip\n",
    "  config.decoder.clip = None\n",
    "  print(\"Decoder conditioned on text\", decoder_text_conditioned)\n",
    "  decoder = config.decoder.create().to(device)\n",
    "  decoder_state_dict = torch.load(decoder_state_dict_path, map_location='cpu')\n",
    "  decoder.load_state_dict(decoder_state_dict, strict=False)\n",
    "  del decoder_state_dict\n",
    "  decoder.eval()\n",
    "  return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed365141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prior(model_path):\n",
    "  prior_network = DiffusionPriorNetwork(\n",
    "    dim=768,\n",
    "    depth=24,\n",
    "    dim_head=64,\n",
    "    heads=32,\n",
    "    normformer=True,\n",
    "    attn_dropout=5e-2,\n",
    "    ff_dropout=5e-2,\n",
    "    num_time_embeds=1,\n",
    "    num_image_embeds=1,\n",
    "    num_text_embeds=1,\n",
    "    num_timesteps=1000,\n",
    "    ff_mult=4\n",
    "  )\n",
    "\n",
    "  diffusion_prior = DiffusionPrior(\n",
    "    net=prior_network,\n",
    "    clip=OpenAIClipAdapter(\"ViT-L/14\"),\n",
    "    image_embed_dim=768,\n",
    "    timesteps=1000,\n",
    "    cond_drop_prob=0.1,\n",
    "    loss_type=\"l2\",\n",
    "    condition_on_text_encodings=True,\n",
    "  ).to(device)\n",
    "\n",
    "  state_dict = torch.load(model_path, map_location='cpu')\n",
    "  if 'ema_model' in state_dict:\n",
    "    print('Loading EMA Model')\n",
    "    diffusion_prior.load_state_dict(state_dict['ema_model'], strict=True)\n",
    "  else:\n",
    "    print('Loading Standard Model')\n",
    "    diffusion_prior.load_state_dict(state_dict['model'], strict=False)\n",
    "  del state_dict\n",
    "  return diffusion_prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05f86667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder conditioned on text True\n",
      "Loading EMA Model\n"
     ]
    }
   ],
   "source": [
    "decoder = load_decoder(current_state[\"model_paths\"][\"decoder\"], current_state[\"model_paths\"][\"decoder_config\"])\n",
    "diffusion_prior = load_prior(current_state[\"model_paths\"][\"prior\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a99de945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "clip = None\n",
    "if clip_config is not None:\n",
    "  clip = clip_config.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6801eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "dalle2 = DALLE2(\n",
    "    prior = diffusion_prior,\n",
    "    decoder = decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16b7c006",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae4a89a4f394e2bbfbc47c311da59c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mdalle2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma butterfly trying to escape a tornado\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2.\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# classifier free guidance strength (> 1 would strengthen the condition)\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py:78\u001b[0m, in \u001b[0;36meval_decorator.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m was_training \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 78\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(was_training)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py:2289\u001b[0m, in \u001b[0;36mDALLE2.forward\u001b[0;34m(self, text, cond_scale, prior_cond_scale, return_pil_images)\u001b[0m\n\u001b[1;32m   2286\u001b[0m image_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior\u001b[38;5;241m.\u001b[39msample(text, num_samples_per_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_num_samples, cond_scale \u001b[38;5;241m=\u001b[39m prior_cond_scale)\n\u001b[1;32m   2288\u001b[0m text_cond \u001b[38;5;241m=\u001b[39m text \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_need_text_cond \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2289\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_pil_images:\n\u001b[1;32m   2292\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_pil, images\u001b[38;5;241m.\u001b[39munbind(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py:78\u001b[0m, in \u001b[0;36meval_decorator.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m was_training \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 78\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(was_training)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py:2142\u001b[0m, in \u001b[0;36mDecoder.sample\u001b[0;34m(self, image_embed, text, text_mask, text_encodings, batch_size, cond_scale, stop_at_unet_number, distributed)\u001b[0m\n\u001b[1;32m   2139\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m image_embed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(text) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(text_encodings) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconditional:\n\u001b[0;32m-> 2142\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip)\n\u001b[1;32m   2143\u001b[0m     _, text_encodings, text_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip\u001b[38;5;241m.\u001b[39membed_text(text)\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcondition_on_text_encodings \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(text_encodings)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext or text encodings must be passed into decoder if specified\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "images = dalle2(\n",
    "    ['a butterfly trying to escape a tornado'],\n",
    "    cond_scale = 2. # classifier free guidance strength (> 1 would strengthen the condition)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bef5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c70cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ad41fdc",
   "metadata": {},
   "source": [
    "### Change conditioned on text to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f74f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditioned_on_text(config):\n",
    "    try:\n",
    "        return config.decoder.unets[0].cond_on_text_encodings\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        return config.decoder.condition_on_text_encodings\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "decoder_text_conditioned = False\n",
    "clip_config = None\n",
    "def load_decoder(decoder_state_dict_path, config_file_path):\n",
    "  config = train_configs.TrainDecoderConfig.from_json_path(config_file_path)\n",
    "  global decoder_text_conditioned\n",
    "  decoder_text_conditioned = conditioned_on_text(config)\n",
    "  print(\"Decoder conditioned on text\", decoder_text_conditioned)\n",
    "  global clip_config\n",
    "  clip_config = config.decoder.clip\n",
    "  config.decoder.clip = None\n",
    "  print(\"Decoder conditioned on text\", decoder_text_conditioned)\n",
    "  decoder = config.decoder.create().to(device)\n",
    "  decoder_state_dict = torch.load(decoder_state_dict_path, map_location='cpu')\n",
    "  decoder.load_state_dict(decoder_state_dict, strict=False)\n",
    "  del decoder_state_dict\n",
    "  decoder.eval()\n",
    "  return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63752f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = train_configs.TrainDecoderConfig.from_json_path(current_state[\"model_paths\"][\"decoder_config\"])\n",
    "decoder_state_dict_path = current_state[\"model_paths\"][\"decoder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cf7ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.decoder.condition_on_text_encodings = False\n",
    "clip_config = config.decoder.clip\n",
    "config.decoder.clip = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2904cbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (unets): ModuleList(\n",
       "    (0): Unet(\n",
       "      (init_conv): CrossEmbedLayer(\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv2d(3, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Conv2d(3, 104, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "          (2): Conv2d(3, 104, kernel_size=(15, 15), stride=(1, 1), padding=(7, 7))\n",
       "        )\n",
       "      )\n",
       "      (to_time_hiddens): Sequential(\n",
       "        (0): SinusoidalPosEmb()\n",
       "        (1): Linear(in_features=416, out_features=1664, bias=True)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (to_time_tokens): Sequential(\n",
       "        (0): Linear(in_features=1664, out_features=1024, bias=True)\n",
       "        (1): Rearrange('b (r d) -> b r d', r=2)\n",
       "      )\n",
       "      (to_time_cond): Sequential(\n",
       "        (0): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "      )\n",
       "      (image_to_tokens): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): Rearrange('b (n d) -> b n d', n=4)\n",
       "      )\n",
       "      (to_image_hiddens): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=1664, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (norm_cond): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_mid_cond): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (downs): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): Conv2d(416, 416, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(416, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): Conv2d(416, 832, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(832, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 832, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): Conv2d(832, 1248, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(1248, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): Conv2d(1248, 1664, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(1664, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 1664, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "      )\n",
       "      (ups): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "            )\n",
       "            (cross_attn): EinopsToAndFrom(\n",
       "              (fn): CrossAttention(\n",
       "                (norm): LayerNorm()\n",
       "                (norm_context): Identity()\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                  (1): LayerNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(3328, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(3328, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(1248, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(1248, 1248, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (cross_attn): EinopsToAndFrom(\n",
       "              (fn): CrossAttention(\n",
       "                (norm): LayerNorm()\n",
       "                (norm_context): Identity()\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                  (1): LayerNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(2496, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(2496, 832, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(832, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 832, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(832, 832, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "            )\n",
       "            (cross_attn): EinopsToAndFrom(\n",
       "              (fn): CrossAttention(\n",
       "                (norm): LayerNorm()\n",
       "                (norm_context): Identity()\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                  (1): LayerNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(1664, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(1664, 416, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(416, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(416, 416, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(832, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(832, 416, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(416, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(416, 416, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (mid_block1): ResnetBlock(\n",
       "        (time_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "        )\n",
       "        (cross_attn): EinopsToAndFrom(\n",
       "          (fn): CrossAttention(\n",
       "            (norm): LayerNorm()\n",
       "            (norm_context): Identity()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (mid_attn): EinopsToAndFrom(\n",
       "        (fn): Residual(\n",
       "          (fn): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=1664, out_features=128, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid_block2): ResnetBlock(\n",
       "        (time_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "        )\n",
       "        (cross_attn): EinopsToAndFrom(\n",
       "          (fn): CrossAttention(\n",
       "            (norm): LayerNorm()\n",
       "            (norm_context): Identity()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Block(\n",
       "            (project): Conv2d(832, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "          (block2): Block(\n",
       "            (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "          (res_conv): Conv2d(832, 416, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2d(416, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vaes): ModuleList(\n",
       "    (0): NullVQGanVAE()\n",
       "  )\n",
       "  (noise_schedulers): ModuleList(\n",
       "    (0): NoiseScheduler()\n",
       "  )\n",
       "  (to_lowres_cond): LowresConditioner()\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = config.decoder.create().to(device)\n",
    "decoder_state_dict = torch.load(decoder_state_dict_path, map_location='cpu')\n",
    "decoder.load_state_dict(decoder_state_dict, strict=False)\n",
    "del decoder_state_dict\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ada96f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee09a4ac20a54b05aa2c5915577110ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mdalle2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma butterfly trying to escape a tornado\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2.\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# classifier free guidance strength (> 1 would strengthen the condition)\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py:78\u001b[0m, in \u001b[0;36meval_decorator.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m was_training \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 78\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(was_training)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py:2289\u001b[0m, in \u001b[0;36mDALLE2.forward\u001b[0;34m(self, text, cond_scale, prior_cond_scale, return_pil_images)\u001b[0m\n\u001b[1;32m   2286\u001b[0m image_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior\u001b[38;5;241m.\u001b[39msample(text, num_samples_per_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_num_samples, cond_scale \u001b[38;5;241m=\u001b[39m prior_cond_scale)\n\u001b[1;32m   2288\u001b[0m text_cond \u001b[38;5;241m=\u001b[39m text \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_need_text_cond \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2289\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_pil_images:\n\u001b[1;32m   2292\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_pil, images\u001b[38;5;241m.\u001b[39munbind(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py:78\u001b[0m, in \u001b[0;36meval_decorator.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m was_training \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 78\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(was_training)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py:2142\u001b[0m, in \u001b[0;36mDecoder.sample\u001b[0;34m(self, image_embed, text, text_mask, text_encodings, batch_size, cond_scale, stop_at_unet_number, distributed)\u001b[0m\n\u001b[1;32m   2139\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m image_embed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(text) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(text_encodings) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconditional:\n\u001b[0;32m-> 2142\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip)\n\u001b[1;32m   2143\u001b[0m     _, text_encodings, text_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip\u001b[38;5;241m.\u001b[39membed_text(text)\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcondition_on_text_encodings \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(text_encodings)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext or text encodings must be passed into decoder if specified\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "images = dalle2(\n",
    "    ['a butterfly trying to escape a tornado'],\n",
    "    cond_scale = 2. # classifier free guidance strength (> 1 would strengthen the condition)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1fbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc1eaa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = train_configs.TrainDecoderConfig.from_json_path(current_state[\"model_paths\"][\"decoder_config\"])\n",
    "decoder_state_dict_path = current_state[\"model_paths\"][\"decoder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7709f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_config = config.decoder.clip\n",
    "clip = None\n",
    "if clip_config:\n",
    "    clip = clip_config.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2651816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (clip): OpenAIClipAdapter(\n",
       "    (clip): CLIP(\n",
       "      (visual): VisionTransformer(\n",
       "        (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (transformer): Transformer(\n",
       "          (resblocks): Sequential(\n",
       "            (0): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (4): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (5): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (6): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (7): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (8): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (9): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (10): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (11): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (12): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (13): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (14): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (15): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (16): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (17): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (18): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (19): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (20): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (21): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (22): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (23): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (clip_normalize): Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       "  )\n",
       "  (unets): ModuleList(\n",
       "    (0): Unet(\n",
       "      (init_conv): CrossEmbedLayer(\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv2d(3, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Conv2d(3, 104, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "          (2): Conv2d(3, 104, kernel_size=(15, 15), stride=(1, 1), padding=(7, 7))\n",
       "        )\n",
       "      )\n",
       "      (to_time_hiddens): Sequential(\n",
       "        (0): SinusoidalPosEmb()\n",
       "        (1): Linear(in_features=416, out_features=1664, bias=True)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (to_time_tokens): Sequential(\n",
       "        (0): Linear(in_features=1664, out_features=1024, bias=True)\n",
       "        (1): Rearrange('b (r d) -> b r d', r=2)\n",
       "      )\n",
       "      (to_time_cond): Sequential(\n",
       "        (0): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "      )\n",
       "      (image_to_tokens): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): Rearrange('b (n d) -> b n d', n=4)\n",
       "      )\n",
       "      (to_image_hiddens): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=1664, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (norm_cond): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_mid_cond): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (downs): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): Conv2d(416, 416, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(416, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): Conv2d(416, 832, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(832, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 832, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): Conv2d(832, 1248, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(1248, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): Conv2d(1248, 1664, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(1664, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 1664, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "      )\n",
       "      (ups): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "            )\n",
       "            (cross_attn): EinopsToAndFrom(\n",
       "              (fn): CrossAttention(\n",
       "                (norm): LayerNorm()\n",
       "                (norm_context): Identity()\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                  (1): LayerNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(3328, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(3328, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(1248, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(1248, 1248, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (cross_attn): EinopsToAndFrom(\n",
       "              (fn): CrossAttention(\n",
       "                (norm): LayerNorm()\n",
       "                (norm_context): Identity()\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                  (1): LayerNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(2496, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(2496, 832, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(832, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 832, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(832, 832, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "            )\n",
       "            (cross_attn): EinopsToAndFrom(\n",
       "              (fn): CrossAttention(\n",
       "                (norm): LayerNorm()\n",
       "                (norm_context): Identity()\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                  (1): LayerNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(1664, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(1664, 416, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(416, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(416, 416, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(832, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(832, 416, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(416, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(416, 416, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (mid_block1): ResnetBlock(\n",
       "        (time_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "        )\n",
       "        (cross_attn): EinopsToAndFrom(\n",
       "          (fn): CrossAttention(\n",
       "            (norm): LayerNorm()\n",
       "            (norm_context): Identity()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (mid_attn): EinopsToAndFrom(\n",
       "        (fn): Residual(\n",
       "          (fn): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=1664, out_features=128, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid_block2): ResnetBlock(\n",
       "        (time_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "        )\n",
       "        (cross_attn): EinopsToAndFrom(\n",
       "          (fn): CrossAttention(\n",
       "            (norm): LayerNorm()\n",
       "            (norm_context): Identity()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Block(\n",
       "            (project): Conv2d(832, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "          (block2): Block(\n",
       "            (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "          (res_conv): Conv2d(832, 416, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2d(416, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vaes): ModuleList(\n",
       "    (0): NullVQGanVAE()\n",
       "  )\n",
       "  (noise_schedulers): ModuleList(\n",
       "    (0): NoiseScheduler()\n",
       "  )\n",
       "  (to_lowres_cond): LowresConditioner()\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = config.decoder.create().to(device)\n",
    "decoder_state_dict = torch.load(decoder_state_dict_path, map_location='cpu')\n",
    "decoder.load_state_dict(decoder_state_dict, strict=False)\n",
    "del decoder_state_dict\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2460d921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1998480b4c504a4688b57d61684bed6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mdalle2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma butterfly trying to escape a tornado\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2.\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# classifier free guidance strength (> 1 would strengthen the condition)\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py:78\u001b[0m, in \u001b[0;36meval_decorator.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m was_training \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 78\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(was_training)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py:2289\u001b[0m, in \u001b[0;36mDALLE2.forward\u001b[0;34m(self, text, cond_scale, prior_cond_scale, return_pil_images)\u001b[0m\n\u001b[1;32m   2286\u001b[0m image_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior\u001b[38;5;241m.\u001b[39msample(text, num_samples_per_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_num_samples, cond_scale \u001b[38;5;241m=\u001b[39m prior_cond_scale)\n\u001b[1;32m   2288\u001b[0m text_cond \u001b[38;5;241m=\u001b[39m text \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_need_text_cond \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2289\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcond_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_pil_images:\n\u001b[1;32m   2292\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_pil, images\u001b[38;5;241m.\u001b[39munbind(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py:78\u001b[0m, in \u001b[0;36meval_decorator.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m was_training \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 78\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(was_training)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.8/site-packages/dalle2_pytorch/dalle2_pytorch.py:2142\u001b[0m, in \u001b[0;36mDecoder.sample\u001b[0;34m(self, image_embed, text, text_mask, text_encodings, batch_size, cond_scale, stop_at_unet_number, distributed)\u001b[0m\n\u001b[1;32m   2139\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m image_embed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(text) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(text_encodings) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconditional:\n\u001b[0;32m-> 2142\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip)\n\u001b[1;32m   2143\u001b[0m     _, text_encodings, text_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip\u001b[38;5;241m.\u001b[39membed_text(text)\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcondition_on_text_encodings \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(text_encodings)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext or text encodings must be passed into decoder if specified\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "images = dalle2(\n",
    "    ['a butterfly trying to escape a tornado'],\n",
    "    cond_scale = 2. # classifier free guidance strength (> 1 would strengthen the condition)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a223f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f245d175",
   "metadata": {},
   "source": [
    "### åŠ äº†conditioned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a334ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = train_configs.TrainDecoderConfig.from_json_path(current_state[\"model_paths\"][\"decoder_config\"])\n",
    "decoder_state_dict_path = current_state[\"model_paths\"][\"decoder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8cbd5f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (clip): OpenAIClipAdapter(\n",
       "    (clip): CLIP(\n",
       "      (visual): VisionTransformer(\n",
       "        (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (transformer): Transformer(\n",
       "          (resblocks): Sequential(\n",
       "            (0): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (4): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (5): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (6): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (7): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (8): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (9): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (10): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (11): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (12): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (13): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (14): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (15): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (16): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (17): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (18): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (19): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (20): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (21): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (22): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (23): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (clip_normalize): Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       "  )\n",
       "  (unets): ModuleList(\n",
       "    (0): Unet(\n",
       "      (init_conv): CrossEmbedLayer(\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv2d(3, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Conv2d(3, 104, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "          (2): Conv2d(3, 104, kernel_size=(15, 15), stride=(1, 1), padding=(7, 7))\n",
       "        )\n",
       "      )\n",
       "      (to_time_hiddens): Sequential(\n",
       "        (0): SinusoidalPosEmb()\n",
       "        (1): Linear(in_features=416, out_features=1664, bias=True)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (to_time_tokens): Sequential(\n",
       "        (0): Linear(in_features=1664, out_features=1024, bias=True)\n",
       "        (1): Rearrange('b (r d) -> b r d', r=2)\n",
       "      )\n",
       "      (to_time_cond): Sequential(\n",
       "        (0): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "      )\n",
       "      (image_to_tokens): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): Rearrange('b (n d) -> b n d', n=4)\n",
       "      )\n",
       "      (to_image_hiddens): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=1664, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (norm_cond): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_mid_cond): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (downs): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): Conv2d(416, 416, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(416, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): Conv2d(416, 832, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(832, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 832, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): Conv2d(832, 1248, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(1248, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): Conv2d(1248, 1664, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(1664, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 1664, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "      )\n",
       "      (ups): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "            )\n",
       "            (cross_attn): EinopsToAndFrom(\n",
       "              (fn): CrossAttention(\n",
       "                (norm): LayerNorm()\n",
       "                (norm_context): Identity()\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                  (1): LayerNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(3328, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(3328, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(1248, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(1248, 1248, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (cross_attn): EinopsToAndFrom(\n",
       "              (fn): CrossAttention(\n",
       "                (norm): LayerNorm()\n",
       "                (norm_context): Identity()\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                  (1): LayerNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(2496, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(2496, 832, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(832, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 832, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(832, 832, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "            )\n",
       "            (cross_attn): EinopsToAndFrom(\n",
       "              (fn): CrossAttention(\n",
       "                (norm): LayerNorm()\n",
       "                (norm_context): Identity()\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                  (1): LayerNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(1664, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(1664, 416, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(416, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(416, 416, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(832, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(832, 416, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(416, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(416, 416, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (mid_block1): ResnetBlock(\n",
       "        (time_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "        )\n",
       "        (cross_attn): EinopsToAndFrom(\n",
       "          (fn): CrossAttention(\n",
       "            (norm): LayerNorm()\n",
       "            (norm_context): Identity()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (mid_attn): EinopsToAndFrom(\n",
       "        (fn): Residual(\n",
       "          (fn): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=1664, out_features=128, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid_block2): ResnetBlock(\n",
       "        (time_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "        )\n",
       "        (cross_attn): EinopsToAndFrom(\n",
       "          (fn): CrossAttention(\n",
       "            (norm): LayerNorm()\n",
       "            (norm_context): Identity()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Block(\n",
       "            (project): Conv2d(832, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "          (block2): Block(\n",
       "            (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "          (res_conv): Conv2d(832, 416, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2d(416, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vaes): ModuleList(\n",
       "    (0): NullVQGanVAE()\n",
       "  )\n",
       "  (noise_schedulers): ModuleList(\n",
       "    (0): NoiseScheduler()\n",
       "  )\n",
       "  (to_lowres_cond): LowresConditioner()\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_text_conditioned = conditioned_on_text(config)\n",
    "print(decoder_text_conditioned)\n",
    "decoder = config.decoder.create().to(device)\n",
    "decoder_state_dict = torch.load(decoder_state_dict_path, map_location='cpu')\n",
    "decoder.load_state_dict(decoder_state_dict, strict=False)\n",
    "del decoder_state_dict\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7ad9a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8017a89480ad49fa9f9808160e1b9f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "texts = ['a bat and a baseball fly through the air']\n",
    "\n",
    "tokens = tokenizer.tokenize(texts).to(device)\n",
    "print(tokens.shape)\n",
    "\n",
    "image_embed = diffusion_prior.sample(tokens, cond_scale = 1)\n",
    "print(image_embed.shape)\n",
    "\n",
    "image_embed = image_embed.cpu().numpy()\n",
    "\n",
    "embeddings = np.repeat(image_embed, 4, axis=0)\n",
    "embeddings = torch.from_numpy(embeddings).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5c7e4be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fc40a8bdb44c6ebcd49a05c378b380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61982fa36bb4ad5b4234174fcdfcbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = tokenizer.tokenize(texts).to(device)\n",
    "text_cond = text if decoder.condition_on_text_encodings else None\n",
    "\n",
    "images = decoder.sample(image_embed = embeddings, text = text_cond, cond_scale = 2)\n",
    "\n",
    "images = images.cpu().permute(0, 2, 3, 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "985941ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb0d5828f40>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "#subplot(r,c) provide the no. of rows and columns\n",
    "f, axarr = plt.subplots(1,4) \n",
    "# use the created array to output your multiple images. In this case I have stacked 4 images vertically\n",
    "axarr[0].imshow(images[0])\n",
    "axarr[1].imshow(images[1])\n",
    "axarr[2].imshow(images[2])\n",
    "axarr[3].imshow(images[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4018a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "568eb818",
   "metadata": {},
   "source": [
    "### cond to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5bf5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = train_configs.TrainDecoderConfig.from_json_path(current_state[\"model_paths\"][\"decoder_config\"])\n",
    "decoder_state_dict_path = current_state[\"model_paths\"][\"decoder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5970528",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.decoder.condition_on_text_encodings = False\n",
    "clip_config = config.decoder.clip\n",
    "config.decoder.clip = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9575758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (unets): ModuleList(\n",
       "    (0): Unet(\n",
       "      (init_conv): CrossEmbedLayer(\n",
       "        (convs): ModuleList(\n",
       "          (0): Conv2d(3, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Conv2d(3, 104, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "          (2): Conv2d(3, 104, kernel_size=(15, 15), stride=(1, 1), padding=(7, 7))\n",
       "        )\n",
       "      )\n",
       "      (to_time_hiddens): Sequential(\n",
       "        (0): SinusoidalPosEmb()\n",
       "        (1): Linear(in_features=416, out_features=1664, bias=True)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (to_time_tokens): Sequential(\n",
       "        (0): Linear(in_features=1664, out_features=1024, bias=True)\n",
       "        (1): Rearrange('b (r d) -> b r d', r=2)\n",
       "      )\n",
       "      (to_time_cond): Sequential(\n",
       "        (0): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "      )\n",
       "      (image_to_tokens): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): Rearrange('b (n d) -> b n d', n=4)\n",
       "      )\n",
       "      (to_image_hiddens): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=1664, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (norm_cond): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_mid_cond): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (downs): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): Conv2d(416, 416, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(416, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): Conv2d(416, 832, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(832, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 832, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): Conv2d(832, 1248, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(1248, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): Conv2d(1248, 1664, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Identity()\n",
       "          )\n",
       "          (2): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(1664, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 1664, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): None\n",
       "        )\n",
       "      )\n",
       "      (ups): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "            )\n",
       "            (cross_attn): EinopsToAndFrom(\n",
       "              (fn): CrossAttention(\n",
       "                (norm): LayerNorm()\n",
       "                (norm_context): Identity()\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                  (1): LayerNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(3328, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(3328, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(1248, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=2496, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=1248, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=1248, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 1248, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(1248, 1248, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (cross_attn): EinopsToAndFrom(\n",
       "              (fn): CrossAttention(\n",
       "                (norm): LayerNorm()\n",
       "                (norm_context): Identity()\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                  (1): LayerNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(2496, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(2496, 832, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(832, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 832, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=832, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=832, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(832, 832, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 832, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(832, 832, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "            )\n",
       "            (cross_attn): EinopsToAndFrom(\n",
       "              (fn): CrossAttention(\n",
       "                (norm): LayerNorm()\n",
       "                (norm_context): Identity()\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                  (1): LayerNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(1664, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(1664, 416, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(416, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (cross_attn): EinopsToAndFrom(\n",
       "                (fn): CrossAttention(\n",
       "                  (norm): LayerNorm()\n",
       "                  (norm_context): Identity()\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (to_q): Linear(in_features=416, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=416, bias=False)\n",
       "                    (1): LayerNorm()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(416, 416, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (time_mlp): Sequential(\n",
       "              (0): SiLU()\n",
       "              (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "            )\n",
       "            (block1): Block(\n",
       "              (project): Conv2d(832, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (block2): Block(\n",
       "              (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (res_conv): Conv2d(832, 416, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): LinearAttention(\n",
       "              (norm): ChanLayerNorm()\n",
       "              (nonlin): GELU(approximate='none')\n",
       "              (to_qkv): Conv2d(416, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Conv2d(512, 416, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ChanLayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (2): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "            (3): ResnetBlock(\n",
       "              (time_mlp): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=1664, out_features=832, bias=True)\n",
       "              )\n",
       "              (block1): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (block2): Block(\n",
       "                (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "                (act): SiLU()\n",
       "              )\n",
       "              (res_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): ConvTranspose2d(416, 416, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (mid_block1): ResnetBlock(\n",
       "        (time_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "        )\n",
       "        (cross_attn): EinopsToAndFrom(\n",
       "          (fn): CrossAttention(\n",
       "            (norm): LayerNorm()\n",
       "            (norm_context): Identity()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (mid_attn): EinopsToAndFrom(\n",
       "        (fn): Residual(\n",
       "          (fn): Attention(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=1664, out_features=128, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid_block2): ResnetBlock(\n",
       "        (time_mlp): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "        )\n",
       "        (cross_attn): EinopsToAndFrom(\n",
       "          (fn): CrossAttention(\n",
       "            (norm): LayerNorm()\n",
       "            (norm_context): Identity()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_q): Linear(in_features=1664, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=1664, bias=False)\n",
       "              (1): LayerNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (project): Conv2d(1664, 1664, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(8, 1664, eps=1e-05, affine=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (res_conv): Identity()\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): ResnetBlock(\n",
       "          (block1): Block(\n",
       "            (project): Conv2d(832, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "          (block2): Block(\n",
       "            (project): Conv2d(416, 416, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm): GroupNorm(8, 416, eps=1e-05, affine=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "          (res_conv): Conv2d(832, 416, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2d(416, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vaes): ModuleList(\n",
       "    (0): NullVQGanVAE()\n",
       "  )\n",
       "  (noise_schedulers): ModuleList(\n",
       "    (0): NoiseScheduler()\n",
       "  )\n",
       "  (to_lowres_cond): LowresConditioner()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_text_conditioned = conditioned_on_text(config)\n",
    "print(decoder_text_conditioned)\n",
    "decoder = config.decoder.create().to(device)\n",
    "decoder_state_dict = torch.load(decoder_state_dict_path, map_location='cpu')\n",
    "decoder.load_state_dict(decoder_state_dict, strict=False)\n",
    "del decoder_state_dict\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3acce7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2eb864282494dfb86391ed0e08c971d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "texts = ['a bat and a baseball fly through the air']\n",
    "\n",
    "tokens = tokenizer.tokenize(texts).to(device)\n",
    "print(tokens.shape)\n",
    "\n",
    "image_embed = diffusion_prior.sample(tokens, cond_scale = 1)\n",
    "print(image_embed.shape)\n",
    "\n",
    "image_embed = image_embed.cpu().numpy()\n",
    "\n",
    "embeddings = np.repeat(image_embed, 4, axis=0)\n",
    "embeddings = torch.from_numpy(embeddings).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8f575bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.condition_on_text_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab14b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.tokenize(texts).to(device)\n",
    "text_cond = text if decoder.condition_on_text_encodings else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb12a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b157901e1684a1786bd749e71b6833e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5fac16a7ac4ff7983369b990b9f2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = tokenizer.tokenize(texts).to(device)\n",
    "text_cond = text if decoder.condition_on_text_encodings else None\n",
    "\n",
    "images = decoder.sample(image_embed = embeddings, text = text_cond, cond_scale = 2)\n",
    "\n",
    "images = images.cpu().permute(0, 2, 3, 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c6c4e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f598048baf0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "#subplot(r,c) provide the no. of rows and columns\n",
    "f, axarr = plt.subplots(1,4) \n",
    "# use the created array to output your multiple images. In this case I have stacked 4 images vertically\n",
    "axarr[0].imshow(images[0])\n",
    "axarr[1].imshow(images[1])\n",
    "axarr[2].imshow(images[2])\n",
    "axarr[3].imshow(images[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255fb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25665908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b0c00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e0214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a5c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d2783aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/songhao.jiang/anaconda3/envs/diffusers/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from typing import Callable, List, Optional, Union\n",
    "import inspect\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e445c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52d76d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 16 files: 100%|█████████████████████| 16/16 [00:00<00:00, 55553.70it/s]\n",
      "/homes/songhao.jiang/anaconda3/envs/diffusers/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a69567f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir imgs\n",
    "!mkdir imgs/homonym_duplication imgs/meaning_edit imgs/meaning_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8920c4",
   "metadata": {},
   "source": [
    "## Getting Images\n",
    "\n",
    "Edited version of the ```StableDiffusionPipeline```'s ```__call__()``` function that enables giving the text embedding directly as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676f8c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(text_embeddings, pipe, img_name,prompt=None, negative_prompt=None,num_images_per_prompt=3):\n",
    "    height = 512\n",
    "    width = 512\n",
    "    num_inference_steps = 50\n",
    "    guidance_scale = 7.5\n",
    "    eta = 0.0\n",
    "    generator = None\n",
    "    latents = None\n",
    "    output_type=\"pil\"\n",
    "    return_dict = True\n",
    "    callback= None\n",
    "    callback_steps= 1\n",
    "    batch_size =1\n",
    "    with torch.no_grad():\n",
    "\n",
    "        bs_embed, seq_len, _ = text_embeddings.shape\n",
    "        text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)\n",
    "        text_embeddings = text_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "        if do_classifier_free_guidance:\n",
    "            uncond_tokens: List[str]\n",
    "            if negative_prompt is None:\n",
    "                uncond_tokens = [\"\"]\n",
    "            elif type(prompt) is not type(negative_prompt):\n",
    "                raise TypeError(\n",
    "                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
    "                    f\" {type(prompt)}.\"\n",
    "                )\n",
    "            elif isinstance(negative_prompt, str):\n",
    "                uncond_tokens = [negative_prompt]\n",
    "            elif batch_size != len(negative_prompt):\n",
    "                raise ValueError(\n",
    "                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
    "                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
    "                    \" the batch size of `prompt`.\"\n",
    "                )\n",
    "            else:\n",
    "                uncond_tokens = negative_prompt\n",
    "\n",
    "            max_length = text_embeddings.shape[1]\n",
    "            uncond_input = pipe.tokenizer(\n",
    "                uncond_tokens,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            uncond_embeddings = pipe.text_encoder(uncond_input.input_ids.to(pipe.device))[0]\n",
    "\n",
    "            seq_len = uncond_embeddings.shape[1]\n",
    "            uncond_embeddings = uncond_embeddings.repeat(batch_size, num_images_per_prompt, 1)\n",
    "            uncond_embeddings = uncond_embeddings.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "        latents_shape = (batch_size * num_images_per_prompt, pipe.unet.in_channels, height // 8, width // 8)\n",
    "        latents_dtype = text_embeddings.dtype\n",
    "        if latents is None:\n",
    "            if pipe.device.type == \"mps\":\n",
    "                latents = torch.randn(latents_shape, generator=generator, device=\"cpu\", dtype=latents_dtype).to(\n",
    "                    pipe.device\n",
    "                )\n",
    "            else:\n",
    "                latents = torch.randn(latents_shape, generator=generator, device=pipe.device, dtype=latents_dtype)\n",
    "        else:\n",
    "            if latents.shape != latents_shape:\n",
    "                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {latents_shape}\")\n",
    "            latents = latents.to(pipe.device)\n",
    "\n",
    "        pipe.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        timesteps_tensor = pipe.scheduler.timesteps.to(pipe.device)\n",
    "\n",
    "        latents = latents * pipe.scheduler.init_noise_sigma\n",
    "\n",
    "        accepts_eta = \"eta\" in set(inspect.signature(pipe.scheduler.step).parameters.keys())\n",
    "        extra_step_kwargs = {}\n",
    "        if accepts_eta:\n",
    "            extra_step_kwargs[\"eta\"] = eta\n",
    "\n",
    "        for i, t in enumerate(pipe.progress_bar(timesteps_tensor)):\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "            latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "            noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "            if do_classifier_free_guidance:\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            latents = pipe.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "            if callback is not None and i % callback_steps == 0:\n",
    "                callback(i, t, latents)\n",
    "\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = pipe.vae.decode(latents).sample\n",
    "\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "\n",
    "        has_nsfw_concept = None\n",
    "\n",
    "        if output_type == \"pil\":\n",
    "            image = pipe.numpy_to_pil(image)\n",
    "\n",
    "        if not return_dict:\n",
    "            print(\"NSFW\")\n",
    "\n",
    "        out=image\n",
    "\n",
    "        for i in range(len(image)):\n",
    "            image[i].save(\"imgs/\"+img_name + \"_\"+str(i)+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503da92",
   "metadata": {},
   "source": [
    "## Editing Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b60684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_b(w, b):\n",
    "    v_b = torch.zeros((768)).type(torch.HalfTensor).cuda()\n",
    "    for j in range(len(b)):\n",
    "        v_b += torch.dot(w,b[j]) * b[j]\n",
    "    return v_b\n",
    "\n",
    "def normal(v):\n",
    "    return (1/torch.sqrt(torch.dot(v,v))) * v\n",
    "\n",
    "def norm(v):\n",
    "    return torch.sqrt(torch.dot(v,v))\n",
    "\n",
    "def project(a, b):\n",
    "    bb_dotprod = torch.dot(b,b)\n",
    "    ab_dotprod = torch.dot(a,b)\n",
    "    if bb_dotprod != 0:\n",
    "        coeff = (ab_dotprod/bb_dotprod)\n",
    "    else:\n",
    "        coeff = 0\n",
    "    return coeff * b\n",
    "\n",
    "def edit_embed(orig_embed, meaning_1, meaning_2):\n",
    "    u = [normal(meaning_1),normal(meaning_2 - project(meaning_2, normal(meaning_1)))]\n",
    "    # pushing ambiguous towards meaning_2\n",
    "    orig_embed = orig_embed  - w_b(orig_embed, u) + norm(meaning_2)*normal(meaning_2 -project(meaning_2, meaning_1)) \n",
    "    return orig_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee59c34",
   "metadata": {},
   "source": [
    "## Getting Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "395d60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_prompt_embed(prompt_1, pipe):\n",
    "    text_inputs = pipe.tokenizer(\n",
    "        prompt_1,\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer.model_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "\n",
    "    text_embeddings_1 = pipe.text_encoder(text_input_ids.to(pipe.device))[0]\n",
    "    \n",
    "    return text_embeddings_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee954e90",
   "metadata": {},
   "source": [
    "## Find Meaning Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f700d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_svd(vectors_m, vectors_n, n, model_dim=768):\n",
    "    mus = [torch.zeros((model_dim)).cuda() for i in range(n)]\n",
    "\n",
    "    for i in range(n):\n",
    "        mus[i] = (1/2)*(vectors_m[i]+vectors_n[i])\n",
    "\n",
    "    subspace = torch.zeros((model_dim,model_dim)).cuda()\n",
    "\n",
    "    for i in range(n):\n",
    "        subspace += (1/2)*torch.outer(vectors_m[i] - mus[i],vectors_m[i]- mus[i])\n",
    "        subspace += (1/2)*torch.outer(vectors_n[i] - mus[i],vectors_n[i]- mus[i])\n",
    "    u_m, s_m, v = np.linalg.svd(subspace.detach().cpu(), full_matrices=True)\n",
    "    return torch.tensor(u_m).type(torch.HalfTensor).cuda(), s_m\n",
    "\n",
    "def find_vectors(w, sentences_1, sentences_2, sentences_amb, pipe, min_dim=10, threshold=0.99, model_dim=768):\n",
    "    n = len(sentences_1)\n",
    "    vectors_1 = []\n",
    "    vectors_2 = []\n",
    "    vectors_amb = []\n",
    "    for i in range(n):\n",
    "        full_vec_1 = one_prompt_embed(sentences_1[i], pipe)\n",
    "        w_idx = sentences_1[i].split(\" \").index(w) + 1\n",
    "        vec_1 = full_vec_1[:,w_idx,:].squeeze(0)\n",
    "        vectors_1.append(vec_1)\n",
    "\n",
    "        full_vec_2 = one_prompt_embed(sentences_2[i], pipe)\n",
    "        w_idx = sentences_2[i].split(\" \").index(w) + 1\n",
    "        vec_2 = full_vec_2[:,w_idx,:].squeeze(0)\n",
    "        vectors_2.append(vec_2)\n",
    "\n",
    "        full_vec_amb = one_prompt_embed(sentences_amb[i], pipe)\n",
    "        w_idx = sentences_amb[i].split(\" \").index(w) + 1\n",
    "        vec_amb = full_vec_amb[:,w_idx,:].squeeze(0)\n",
    "        vectors_amb.append(vec_amb)\n",
    "\n",
    "    u_1, s_1 = diff_svd(vectors_1, vectors_amb, n, model_dim)\n",
    "    u_2, s_2 = diff_svd(vectors_2, vectors_amb, n, model_dim)\n",
    "\n",
    "    dim = 0\n",
    "    while sum(s_1[:dim])/sum(s_1) < threshold or sum(s_2[:dim])/sum(s_2) < threshold or dim < min_dim:\n",
    "        dim += 1\n",
    "    u_b_1 = [u_1[:,j] for j in range(dim)]\n",
    "    u_b_2 = [u_2[:,j] for j in range(dim)]\n",
    "\n",
    "    diff_1 = [normal(w_b(vectors_1[i], u_b_1)) for i in range(n)]\n",
    "    diff_2 = [normal(w_b(vectors_2[i], u_b_2)) for i in range(n)]\n",
    "    diff_amb_1 = [normal(w_b(vectors_amb[i], u_b_1)) for i in range(n)]\n",
    "    diff_amb_2 = [normal(w_b(vectors_amb[i], u_b_2)) for i in range(n)]\n",
    "\n",
    "    v_1 = torch.zeros((model_dim)).type(torch.HalfTensor).cuda()\n",
    "    v_2 = torch.zeros((model_dim)).type(torch.HalfTensor).cuda()\n",
    "    for i in range(dim):\n",
    "        v_1 += sum([torch.dot(diff_1[j]  , u_b_1[i]) for j in range(n)])/n * u_b_1[i] \n",
    "        v_2 += sum([torch.dot(diff_2[j] , u_b_2[i]) for j in range(n)])/n * u_b_2[i]\n",
    "\n",
    "    for i in range(n):\n",
    "        v_1 = v_1 - project(v_1, normal(vectors_2[i]))\n",
    "        v_2 = v_2 - project(v_2, normal(vectors_1[i]))\n",
    "\n",
    "    norm_v_1 = norm(v_1)\n",
    "    v_1 = normal(v_1)\n",
    "\n",
    "    norm_v_2 = norm(v_2)\n",
    "    v_2 = normal(v_2)\n",
    "    for i in range(n):\n",
    "        proj_1 = vectors_1[i]\n",
    "        proj_2 = vectors_2[i]\n",
    "        proj_amb = vectors_amb[i]\n",
    "    return max([torch.dot(vectors_1[j] , v_1) for j in range(n)]) *v_1, max([torch.dot(vectors_2[j] , v_2) for j in range(n)]) *v_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dcf995",
   "metadata": {},
   "source": [
    "## Generate All Images for Sense Editing Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa284bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_prompts(word, prompt_dict, sentences_1, sentences_2, sentences_amb, pipe, neg_prompt=\"\", repeat=5):\n",
    "    v_1, v_2 = find_vectors(word, sentences_1, sentences_2, sentences_amb, pipe, threshold=0.95,min_dim=3)\n",
    "    for prompt, filename in prompt_dict.items():\n",
    "        orig_prompt = prompt\n",
    "        orig_embed = one_prompt_embed(orig_prompt,pipe)\n",
    "        idx = orig_prompt.split(\" \").index(word) + 1\n",
    "\n",
    "        embed_1 = orig_embed.detach().clone()\n",
    "        embed_1[:,idx,:] = edit_embed(embed_1[:,idx,:].squeeze(0).clone(), v_2, v_1).clone()\n",
    "\n",
    "        embed_2 = orig_embed.detach().clone()\n",
    "        embed_2[:,idx,:] = edit_embed(embed_2[:,idx,:].squeeze(0).clone(), v_1, v_2).clone()\n",
    "        \n",
    "        for i in range(repeat):\n",
    "            get_images(embed_1, pipe, filename + \"sense_1_\" + str(i))\n",
    "            get_images(embed_2, pipe, filename + \"sense_2_\" + str(i))\n",
    "            get_images(orig_embed, pipe, filename + \"amb_\" + str(i))\n",
    "        if neg_prompt != \"\":\n",
    "            for i in range(repeat):\n",
    "                get_images(embed_1, pipe, filename + \"sense_1_\" + str(i)+\"_neg\", prompt = orig_prompt, negative_prompt=neg_prompt)\n",
    "                get_images(embed_2, pipe, filename + \"sense_2_\" + str(i)+\"_neg\", prompt = orig_prompt, negative_prompt=neg_prompt)\n",
    "                get_images(orig_embed, pipe, filename + \"amb_\" + str(i)+\"_neg\", prompt = orig_prompt, negative_prompt=neg_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "795e296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bass_sentence_music = [\"the musician played a double bass\"]\n",
    "\n",
    "bass_sentence_fish = [\"the fisherman caught a sea bass\"]\n",
    "\n",
    "bass_sentence_amb = [\"a bass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70246036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 51/51 [00:10<00:00,  4.85it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:10<00:00,  4.75it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:10<00:00,  4.85it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:10<00:00,  4.84it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:10<00:00,  4.84it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:10<00:00,  4.83it/s]\n"
     ]
    }
   ],
   "source": [
    "edit_prompts(\"bass\", {\"a bass\":\"meaning_edit/bass_\"}, bass_sentence_music, bass_sentence_fish, bass_sentence_amb, pipe, neg_prompt=\"disfigured, deformed, bad anatomy, low quality, jpeg artifacts\", repeat=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6694203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bf1775fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "word, prompt_dict, sentences_1, sentences_2, sentences_amb, pipe = \"bass\", {\"a bass\":\"meaning_edit/bass_\"}, bass_sentence_music, bass_sentence_fish, bass_sentence_amb, pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b1e923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_1, v_2 = find_vectors(word, sentences_1, sentences_2, sentences_amb, pipe, threshold=0.95,min_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11eb246d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "06ef5914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "30fe60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(sentences_1)\n",
    "vectors_1 = []\n",
    "vectors_2 = []\n",
    "vectors_amb = []\n",
    "for i in range(n):\n",
    "    full_vec_1 = one_prompt_embed(sentences_1[i], pipe)\n",
    "    w_idx = sentences_1[i].split(\" \").index(word) + 1\n",
    "    vec_1 = full_vec_1[:,w_idx,:].squeeze(0)\n",
    "    vectors_1.append(vec_1)\n",
    "\n",
    "    full_vec_2 = one_prompt_embed(sentences_2[i], pipe)\n",
    "    w_idx = sentences_2[i].split(\" \").index(word) + 1\n",
    "    vec_2 = full_vec_2[:,w_idx,:].squeeze(0)\n",
    "    vectors_2.append(vec_2)\n",
    "\n",
    "    full_vec_amb = one_prompt_embed(sentences_amb[i], pipe)\n",
    "    w_idx = sentences_amb[i].split(\" \").index(word) + 1\n",
    "    vec_amb = full_vec_amb[:,w_idx,:].squeeze(0)\n",
    "    vectors_amb.append(vec_amb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ef08d1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "85fbf34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vec_2 = one_prompt_embed(sentences_2[i], pipe)\n",
    "w_idx = sentences_2[i].split(\" \").index(word) + 1\n",
    "vec_2 = full_vec_2[:,w_idx,:].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b1f409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb16cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70b8880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/songhao.jiang/anaconda3/envs/diffusers/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from typing import Callable, List, Optional, Union\n",
    "import inspect\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ed5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.sensepolar_embeddings import SensePolar\n",
    "from scripts.SensePOLAR import bertFuncs as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410032cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c354e576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = func.getBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b16ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"bat\"\n",
    "context_1 = \"a baseball player swings a baseball bat\"\n",
    "context_2 = \"a fruit bat is hanging from the tree\"\n",
    "context_amb = \"a bat in a box\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6ea4a",
   "metadata": {},
   "source": [
    "## sensePOLAR embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74c53fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1762\n",
      "Top:  0\n",
      "Dimension:  guided<------>unguided\n",
      "Definitions:  subject to guidance or control especially after launching<------>not subject to guidance or control after launching\n",
      "Value: -0.1434815\n",
      "\n",
      "\n",
      "Top:  1\n",
      "Dimension:  conclusive<------>inconclusive\n",
      "Definitions:  forming an end or termination; especially putting an end to doubt or question<------>not conclusive; not putting an end to doubt or question\n",
      "Value: -0.090800114\n",
      "\n",
      "\n",
      "Top:  2\n",
      "Dimension:  unwanted<------>wanted\n",
      "Definitions:  not wanted; not needed<------>desired or wished for or sought\n",
      "Value: -0.08917418\n",
      "\n",
      "\n",
      "Top:  3\n",
      "Dimension:  binaural<------>monaural\n",
      "Definitions:  relating to or having or hearing with two ears<------>relating to or having or hearing with only one ear\n",
      "Value:                      0.08604479\n",
      "\n",
      "\n",
      "Top:  4\n",
      "Dimension:  dislike<------>liking\n",
      "Definitions:  a feeling of aversion or antipathy<------>a feeling of pleasure and enjoyment\n",
      "Value:                      0.0852669\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SensePOLAR_embed_1 = SensePolar(tokenizer, model, context_1, word, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5ced024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1762\n",
      "Top:  0\n",
      "Dimension:  eyed<------>eyeless\n",
      "Definitions:  having an eye or eyes or eyelike feature especially as specified; often used in combination<------>lacking eyes or eyelike features\n",
      "Value:                      0.101783946\n",
      "\n",
      "\n",
      "Top:  1\n",
      "Dimension:  feathered<------>unfeathered\n",
      "Definitions:  having or covered with feathers<------>having no feathers\n",
      "Value: -0.09848488\n",
      "\n",
      "\n",
      "Top:  2\n",
      "Dimension:  righteous<------>unrighteous\n",
      "Definitions:  characterized by or proceeding from accepted standards of morality or justice; - James 5:16<------>not righteous\n",
      "Value: -0.09049017\n",
      "\n",
      "\n",
      "Top:  3\n",
      "Dimension:  establish<------>abolish\n",
      "Definitions:  do away with<------>set up or found\n",
      "Value: -0.08902636\n",
      "\n",
      "\n",
      "Top:  4\n",
      "Dimension:  lidded<------>lidless\n",
      "Definitions:  having or covered with a lid or lids; often used in combination<------>not having or covered with a lid or lids\n",
      "Value:                      0.08796764\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SensePOLAR_embed_2 = SensePolar(tokenizer, model, context_2, word, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19acec81",
   "metadata": {},
   "source": [
    "## CLIP embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8cf99f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 16 files: 100%|████████████████████| 16/16 [00:00<00:00, 160932.53it/s]\n",
      "/homes/songhao.jiang/anaconda3/envs/diffusers/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b85d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting encodings from CLIPS\n",
    "def one_prompt_embed(prompt_1, pipe):\n",
    "    text_inputs = pipe.tokenizer(\n",
    "        prompt_1,\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer.model_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "\n",
    "    text_embeddings_1 = pipe.text_encoder(text_input_ids.to(pipe.device))[0]\n",
    "    \n",
    "    return text_embeddings_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf2b2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vec_1 = one_prompt_embed(context_1, pipe)\n",
    "w_idx = context_1.split(\" \").index(word) + 1\n",
    "CLIPS_embed_1 = full_vec_1[:,w_idx,:].squeeze(0)\n",
    "\n",
    "full_vec_2 = one_prompt_embed(context_2, pipe)\n",
    "w_idx = context_2.split(\" \").index(word) + 1\n",
    "CLIPS_embed_2 = full_vec_2[:,w_idx,:].squeeze(0)\n",
    "\n",
    "full_vec_amb = one_prompt_embed(context_amb, pipe)\n",
    "w_idx = context_amb.split(\" \").index(word) + 1\n",
    "CLIPS_embed_amb = full_vec_amb[:,w_idx,:].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "409c0237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768]), (1762,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLIPS_embed_1.shape, SensePOLAR_embed_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a8bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d016c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(text_embeddings, pipe, img_name,prompt=None, negative_prompt=None,num_images_per_prompt=3):\n",
    "    height = 512\n",
    "    width = 512\n",
    "    num_inference_steps = 50\n",
    "    guidance_scale = 7.5\n",
    "    eta = 0.0\n",
    "    generator = None\n",
    "    latents = None\n",
    "    output_type=\"pil\"\n",
    "    return_dict = True\n",
    "    callback= None\n",
    "    callback_steps= 1\n",
    "    batch_size =1\n",
    "    with torch.no_grad():\n",
    "\n",
    "        bs_embed, seq_len, _ = text_embeddings.shape\n",
    "        text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)\n",
    "        text_embeddings = text_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "        if do_classifier_free_guidance:\n",
    "            uncond_tokens: List[str]\n",
    "            if negative_prompt is None:\n",
    "                uncond_tokens = [\"\"]\n",
    "            elif type(prompt) is not type(negative_prompt):\n",
    "                raise TypeError(\n",
    "                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
    "                    f\" {type(prompt)}.\"\n",
    "                )\n",
    "            elif isinstance(negative_prompt, str):\n",
    "                uncond_tokens = [negative_prompt]\n",
    "            elif batch_size != len(negative_prompt):\n",
    "                raise ValueError(\n",
    "                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
    "                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
    "                    \" the batch size of `prompt`.\"\n",
    "                )\n",
    "            else:\n",
    "                uncond_tokens = negative_prompt\n",
    "\n",
    "            max_length = text_embeddings.shape[1]\n",
    "            uncond_input = pipe.tokenizer(\n",
    "                uncond_tokens,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            uncond_embeddings = pipe.text_encoder(uncond_input.input_ids.to(pipe.device))[0]\n",
    "\n",
    "            seq_len = uncond_embeddings.shape[1]\n",
    "            uncond_embeddings = uncond_embeddings.repeat(batch_size, num_images_per_prompt, 1)\n",
    "            uncond_embeddings = uncond_embeddings.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "        latents_shape = (batch_size * num_images_per_prompt, pipe.unet.in_channels, height // 8, width // 8)\n",
    "        latents_dtype = text_embeddings.dtype\n",
    "        if latents is None:\n",
    "            if pipe.device.type == \"mps\":\n",
    "                latents = torch.randn(latents_shape, generator=generator, device=\"cpu\", dtype=latents_dtype).to(\n",
    "                    pipe.device\n",
    "                )\n",
    "            else:\n",
    "                latents = torch.randn(latents_shape, generator=generator, device=pipe.device, dtype=latents_dtype)\n",
    "        else:\n",
    "            if latents.shape != latents_shape:\n",
    "                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {latents_shape}\")\n",
    "            latents = latents.to(pipe.device)\n",
    "\n",
    "        pipe.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        timesteps_tensor = pipe.scheduler.timesteps.to(pipe.device)\n",
    "\n",
    "        latents = latents * pipe.scheduler.init_noise_sigma\n",
    "\n",
    "        accepts_eta = \"eta\" in set(inspect.signature(pipe.scheduler.step).parameters.keys())\n",
    "        extra_step_kwargs = {}\n",
    "        if accepts_eta:\n",
    "            extra_step_kwargs[\"eta\"] = eta\n",
    "\n",
    "        for i, t in enumerate(pipe.progress_bar(timesteps_tensor)):\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "            latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "            noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "            if do_classifier_free_guidance:\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            latents = pipe.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "            if callback is not None and i % callback_steps == 0:\n",
    "                callback(i, t, latents)\n",
    "\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = pipe.vae.decode(latents).sample\n",
    "\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "\n",
    "        has_nsfw_concept = None\n",
    "\n",
    "        if output_type == \"pil\":\n",
    "            image = pipe.numpy_to_pil(image)\n",
    "\n",
    "        if not return_dict:\n",
    "            print(\"NSFW\")\n",
    "\n",
    "        out=image\n",
    "\n",
    "        for i in range(len(image)):\n",
    "            image[i].save(\"imgs/\"+img_name + \"_\"+str(i)+\".png\")\n",
    "            \n",
    "def w_b(w, b):\n",
    "    v_b = torch.zeros((768)).type(torch.HalfTensor).cuda()\n",
    "    for j in range(len(b)):\n",
    "        v_b += torch.dot(w,b[j]) * b[j]\n",
    "    return v_b\n",
    "\n",
    "def normal(v):\n",
    "    return (1/torch.sqrt(torch.dot(v,v))) * v\n",
    "\n",
    "def norm(v):\n",
    "    return torch.sqrt(torch.dot(v,v))\n",
    "\n",
    "def project(a, b):\n",
    "    bb_dotprod = torch.dot(b,b)\n",
    "    ab_dotprod = torch.dot(a,b)\n",
    "    if bb_dotprod != 0:\n",
    "        coeff = (ab_dotprod/bb_dotprod)\n",
    "    else:\n",
    "        coeff = 0\n",
    "    return coeff * b\n",
    "\n",
    "def edit_embed(orig_embed, meaning_1, meaning_2):\n",
    "    u = [normal(meaning_1),normal(meaning_2 - project(meaning_2, normal(meaning_1)))]\n",
    "    # pushing ambiguous towards meaning_2\n",
    "    orig_embed = orig_embed  - w_b(orig_embed, u) + norm(meaning_2)*normal(meaning_2 -project(meaning_2, meaning_1)) \n",
    "    return orig_embed\n",
    "\n",
    "def diff_svd(vectors_m, vectors_n, n, model_dim=768):\n",
    "    mus = [torch.zeros((model_dim)).cuda() for i in range(n)]\n",
    "\n",
    "    for i in range(n):\n",
    "        mus[i] = (1/2)*(vectors_m[i]+vectors_n[i])\n",
    "\n",
    "    subspace = torch.zeros((model_dim,model_dim)).cuda()\n",
    "\n",
    "    for i in range(n):\n",
    "        subspace += (1/2)*torch.outer(vectors_m[i] - mus[i],vectors_m[i]- mus[i])\n",
    "        subspace += (1/2)*torch.outer(vectors_n[i] - mus[i],vectors_n[i]- mus[i])\n",
    "    u_m, s_m, v = np.linalg.svd(subspace.detach().cpu(), full_matrices=True)\n",
    "    return torch.tensor(u_m).type(torch.HalfTensor).cuda(), s_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "778a0dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc791755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "667845b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vectors_from_embed(w, embed_1, embed_2, embed_amb, pipe, min_dim=10, threshold=0.99, model_dim=768):\n",
    "    u_1, s_1 = diff_svd(vectors_1, vectors_amb, 1, model_dim)\n",
    "    u_2, s_2 = diff_svd(vectors_2, vectors_amb, 1, model_dim)\n",
    "    \n",
    "    dim = 0\n",
    "    while sum(s_1[:dim])/sum(s_1) < threshold or sum(s_2[:dim])/sum(s_2) < threshold or dim < min_dim:\n",
    "        dim += 1\n",
    "    u_b_1 = [u_1[:,j] for j in range(dim)]\n",
    "    u_b_2 = [u_2[:,j] for j in range(dim)]\n",
    "\n",
    "    diff_1 = [normal(w_b(vectors_1[i], u_b_1)) for i in range(n)]\n",
    "    diff_2 = [normal(w_b(vectors_2[i], u_b_2)) for i in range(n)]\n",
    "    diff_amb_1 = [normal(w_b(vectors_amb[i], u_b_1)) for i in range(n)]\n",
    "    diff_amb_2 = [normal(w_b(vectors_amb[i], u_b_2)) for i in range(n)]\n",
    "\n",
    "    v_1 = torch.zeros((model_dim)).type(torch.HalfTensor).cuda()\n",
    "    v_2 = torch.zeros((model_dim)).type(torch.HalfTensor).cuda()\n",
    "    for i in range(dim):\n",
    "        v_1 += sum([torch.dot(diff_1[j]  , u_b_1[i]) for j in range(n)])/n * u_b_1[i] \n",
    "        v_2 += sum([torch.dot(diff_2[j] , u_b_2[i]) for j in range(n)])/n * u_b_2[i]\n",
    "\n",
    "    for i in range(n):\n",
    "        v_1 = v_1 - project(v_1, normal(vectors_2[i]))\n",
    "        v_2 = v_2 - project(v_2, normal(vectors_1[i]))\n",
    "\n",
    "    norm_v_1 = norm(v_1)\n",
    "    v_1 = normal(v_1)\n",
    "\n",
    "    norm_v_2 = norm(v_2)\n",
    "    v_2 = normal(v_2)\n",
    "    for i in range(n):\n",
    "        proj_1 = vectors_1[i]\n",
    "        proj_2 = vectors_2[i]\n",
    "        proj_amb = vectors_amb[i]\n",
    "    return max([torch.dot(vectors_1[j] , v_1) for j in range(n)]) *v_1, max([torch.dot(vectors_2[j] , v_2) for j in range(n)]) *v_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c202d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_prompts(word, prompt_dict, sentences_1, sentences_2, sentences_amb, pipe, neg_prompt=\"\", repeat=5):\n",
    "    v_1, v_2 = find_vectors_from_embed(word, sentences_1, sentences_2, sentences_amb, pipe, threshold=0.95,min_dim=3)\n",
    "    for prompt, filename in prompt_dict.items():\n",
    "        orig_prompt = prompt\n",
    "        orig_embed = one_prompt_embed(orig_prompt,pipe)\n",
    "        idx = orig_prompt.split(\" \").index(word) + 1\n",
    "\n",
    "        embed_1 = orig_embed.detach().clone()\n",
    "        embed_1[:,idx,:] = edit_embed(embed_1[:,idx,:].squeeze(0).clone(), v_2, v_1).clone()\n",
    "\n",
    "        embed_2 = orig_embed.detach().clone()\n",
    "        embed_2[:,idx,:] = edit_embed(embed_2[:,idx,:].squeeze(0).clone(), v_1, v_2).clone()\n",
    "        \n",
    "        for i in range(repeat):\n",
    "            get_images(embed_1, pipe, filename + \"sense_1_\" + str(i))\n",
    "            get_images(embed_2, pipe, filename + \"sense_2_\" + str(i))\n",
    "            get_images(orig_embed, pipe, filename + \"amb_\" + str(i))\n",
    "        if neg_prompt != \"\":\n",
    "            for i in range(repeat):\n",
    "                get_images(embed_1, pipe, filename + \"sense_1_\" + str(i)+\"_neg\", prompt = orig_prompt, negative_prompt=neg_prompt)\n",
    "                get_images(embed_2, pipe, filename + \"sense_2_\" + str(i)+\"_neg\", prompt = orig_prompt, negative_prompt=neg_prompt)\n",
    "                get_images(orig_embed, pipe, filename + \"amb_\" + str(i)+\"_neg\", prompt = orig_prompt, negative_prompt=neg_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc38b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_1 = [torch.from_numpy(SensePOLAR_embed_1[:768]).type('torch.HalfTensor').cuda()]\n",
    "vectors_2 = [torch.from_numpy(SensePOLAR_embed_2[:768]).type('torch.HalfTensor').cuda()]\n",
    "vectors_amb = [CLIPS_embed_amb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1761bfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 51/51 [00:11<00:00,  4.39it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:10<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:10<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:10<00:00,  4.86it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:10<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:10<00:00,  4.85it/s]\n"
     ]
    }
   ],
   "source": [
    "edit_prompts(\"bat\", {\"a bat and a baseball fly through the air\":\"test/bat_fly_through_the_air_\"}, \\\n",
    "             vectors_1, vectors_2, vectors_amb, pipe, neg_prompt=\"disfigured, deformed, bad anatomy, low quality, jpeg artifacts\", repeat=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e6f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
